{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP+Z7zGmAkl8duxYT23UHBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kopyshevskiy/AN2DL_ch2/blob/main/unet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\")\n",
        "%cd /gdrive/My Drive/AN2DL/Homework2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQvTvGUv2sRM",
        "outputId": "53272e5e-f614-444f-93d7-af032d8bf91d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/AN2DL/Homework2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wnbJh7H1yv2",
        "outputId": "640a72f1-b265-404b-9e23-0bdaabaa6126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# Set seed for TensorFlow\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "# Reduce TensorFlow verbosity\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Print TensorFlow version\n",
        "print(tf.__version__)\n",
        "\n",
        "# Import other libraries\n",
        "import os\n",
        "import math\n",
        "from PIL import Image\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data = np.load(\"mars_for_students.npz\")\n",
        "\n",
        "training_set = data[\"training_set\"]\n",
        "X_train = training_set[:, 0]\n",
        "y_train = training_set[:, 1]\n",
        "\n",
        "X_test = data[\"test_set\"]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val   = X_val.astype(np.float32)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "\n",
        "\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_val   = y_val.astype(np.int32)\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "y_val   = np.expand_dims(y_val, axis=-1)\n",
        "\n",
        "print(f\" X_train: {X_train.shape}\")\n",
        "print(f\" y_train: {y_train.shape}\")\n",
        "print(f\" X_val:   {X_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnRz-fSG3G6g",
        "outputId": "e47dd4a1-3d95-42b7-d4bd-e3e2c3002512"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " X_train: (2092, 64, 128, 1)\n",
            " y_train: (2092, 64, 128, 1)\n",
            " X_val:   (523, 64, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def geometric_augmentation(X, y, augmenter, num_augmentations=1):\n",
        "\n",
        "    augmented_X, augmented_y = [], []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "\n",
        "        img = X[i].reshape((1,) + X[i].shape)\n",
        "        mask = y[i].reshape((1,) + y[i].shape)\n",
        "\n",
        "        for _ in range(num_augmentations):\n",
        "\n",
        "            seed = np.random.randint(1e6)\n",
        "\n",
        "            img_flow = augmenter.flow(img,   batch_size=1, seed=seed)\n",
        "            mask_flow = augmenter.flow(mask, batch_size=1, seed=seed)\n",
        "\n",
        "            aug_img  = next(img_flow)[0]\n",
        "            aug_mask = next(mask_flow)[0]\n",
        "\n",
        "            augmented_X.append(aug_img)\n",
        "            augmented_y.append(aug_mask)\n",
        "\n",
        "    return np.array(augmented_X), np.array(augmented_y)\n"
      ],
      "metadata": {
        "id": "DG6GEgzj6_gZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    width_shift_range = 0.1,\n",
        "    height_shift_range = 0.1,\n",
        "    shear_range = 0.1,\n",
        "    zoom_range = 0.1,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\n",
        "X_train, y_train = geometric_augmentation(X_train, y_train, augmenter, num_augmentations = 5)\n",
        "\n",
        "print(f\" X_train: {X_train.shape}\")\n",
        "print(f\" y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDydmwDTLLK6",
        "outputId": "5afc7a6e-8f8f-468b-84dd-b72cb67136a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " X_train: (10460, 64, 128, 1)\n",
            " y_train: (10460, 64, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def photometric_augmentation(X, y, augmenter, num_augmentations=1):\n",
        "\n",
        "    augmented_X, augmented_y = [], []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        img = X[i].reshape((1,) + X[i].shape)\n",
        "        label = y[i]\n",
        "\n",
        "        for _ in range(num_augmentations):\n",
        "            for augmented_img in augmenter.flow(img, batch_size=1):\n",
        "                augmented_X.append(augmented_img[0])\n",
        "                augmented_y.append(label)\n",
        "                break\n",
        "\n",
        "    return np.array(augmented_X), np.array(augmented_y)"
      ],
      "metadata": {
        "id": "d45yZWKVIV-j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmenter = ImageDataGenerator(\n",
        "    brightness_range=[0.2, 1.8],\n",
        ")\n",
        "\n",
        "X_train, y_train = photometric_augmentation(X_train, y_train, augmenter, num_augmentations = 1)\n",
        "\n",
        "print(f\" X_train: {X_train.shape}\")\n",
        "print(f\" y_train: {y_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htv2dED6N40y",
        "outputId": "471fd90f-816b-4cef-dff5-1cd8a726d6a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " X_train: (10460, 64, 128, 1)\n",
            " y_train: (10460, 64, 128, 1)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models, Input\n",
        "\n",
        "\n",
        "# def residual_block(x, filters, dropout_rate=0.0):\n",
        "#     shortcut = x\n",
        "\n",
        "#     # First Conv -> BN -> ReLU\n",
        "#     x = layers.Conv2D(filters, 3, padding='same', kernel_initializer='he_normal')(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "#     x = layers.ReLU()(x)\n",
        "\n",
        "#     # Second Conv -> BN\n",
        "#     x = layers.Conv2D(filters, 3, padding='same', kernel_initializer='he_normal')(x)\n",
        "#     x = layers.BatchNormalization()(x)\n",
        "\n",
        "#     # Add the shortcut connection\n",
        "#     # Adjust the number of filters in the shortcut to match x\n",
        "#     shortcut = layers.Conv2D(filters, 1, padding='same', kernel_initializer='he_normal')(shortcut)\n",
        "#     x = layers.Add()([x, shortcut])\n",
        "#     x = layers.ReLU()(x)\n",
        "\n",
        "#     # Optional Dropout\n",
        "#     if dropout_rate > 0.0:\n",
        "#         x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "#     return x\n",
        "\n",
        "\n",
        "# def unet_model_with_bn_rb():\n",
        "#     inputs = Input(shape=(64, 128, 1))\n",
        "\n",
        "#     # Encoder with Residual Blocks\n",
        "#     conv1 = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
        "#     conv1 = residual_block(conv1, 64, dropout_rate=0.1)\n",
        "#     mpool1 = layers.MaxPool2D()(conv1)\n",
        "\n",
        "#     conv2 = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(mpool1)\n",
        "#     conv2 = residual_block(conv2, 128, dropout_rate=0.2)\n",
        "#     mpool2 = layers.MaxPool2D()(conv2)\n",
        "\n",
        "#     conv3 = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(mpool2)\n",
        "#     conv3 = residual_block(conv3, 256, dropout_rate=0.3)\n",
        "#     mpool3 = layers.MaxPool2D()(conv3)\n",
        "\n",
        "#     conv4 = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(mpool3)\n",
        "#     conv4 = residual_block(conv4, 512, dropout_rate=0.4)\n",
        "#     mpool4 = layers.MaxPool2D()(conv4)\n",
        "\n",
        "#     # Bottleneck\n",
        "#     conv5 = layers.Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(mpool4)\n",
        "#     conv5 = residual_block(conv5, 1024, dropout_rate=0.5)\n",
        "\n",
        "#     # Decoder with Residual Blocks\n",
        "#     up6 = layers.Conv2DTranspose(512, 2, strides=2, kernel_initializer='he_normal', padding='same')(conv5)\n",
        "#     conv6 = layers.Concatenate()([up6, conv4])\n",
        "#     conv6 = residual_block(conv6, 512, dropout_rate=0.4)\n",
        "\n",
        "#     up7 = layers.Conv2DTranspose(256, 2, strides=2, kernel_initializer='he_normal', padding='"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4vCSBF8PnFFW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input\n",
        "\n",
        "def unet_model_with_bn_rb():\n",
        "    inputs = Input(shape=(64, 128, 1))\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.ReLU()(conv1)\n",
        "    conv1 = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv1)\n",
        "    conv1 = layers.BatchNormalization()(conv1)\n",
        "    conv1 = layers.Add()([layers.Conv2D(64, 1, padding='same', kernel_initializer='he_normal')(inputs), conv1])  # Match channels\n",
        "    conv1 = layers.ReLU()(conv1)\n",
        "    conv1 = layers.Dropout(0.1)(conv1)\n",
        "    mpool1 = layers.MaxPool2D()(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(mpool1)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    conv2 = layers.ReLU()(conv2)\n",
        "    conv2 = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv2)\n",
        "    conv2 = layers.BatchNormalization()(conv2)\n",
        "    conv2 = layers.Add()([layers.Conv2D(128, 1, padding='same', kernel_initializer='he_normal')(mpool1), conv2])  # Match channels\n",
        "    conv2 = layers.ReLU()(conv2)\n",
        "    conv2 = layers.Dropout(0.2)(conv2)\n",
        "    mpool2 = layers.MaxPool2D()(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(mpool2)\n",
        "    conv3 = layers.BatchNormalization()(conv3)\n",
        "    conv3 = layers.ReLU()(conv3)\n",
        "    conv3 = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv3)\n",
        "    conv3 = layers.BatchNormalization()(conv3)\n",
        "    conv3 = layers.Add()([layers.Conv2D(256, 1, padding='same', kernel_initializer='he_normal')(mpool2), conv3])  # Match channels\n",
        "    conv3 = layers.ReLU()(conv3)\n",
        "    conv3 = layers.Dropout(0.3)(conv3)\n",
        "    mpool3 = layers.MaxPool2D()(conv3)\n",
        "\n",
        "    conv4 = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(mpool3)\n",
        "    conv4 = layers.BatchNormalization()(conv4)\n",
        "    conv4 = layers.ReLU()(conv4)\n",
        "    conv4 = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv4)\n",
        "    conv4 = layers.BatchNormalization()(conv4)\n",
        "    conv4 = layers.Add()([layers.Conv2D(512, 1, padding='same', kernel_initializer='he_normal')(mpool3), conv4])  # Match channels\n",
        "    conv4 = layers.ReLU()(conv4)\n",
        "    conv4 = layers.Dropout(0.4)(conv4)\n",
        "    mpool4 = layers.MaxPool2D()(conv4)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv5 = layers.Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(mpool4)\n",
        "    conv5 = layers.BatchNormalization()(conv5)\n",
        "    conv5 = layers.ReLU()(conv5)\n",
        "    conv5 = layers.Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(conv5)\n",
        "    conv5 = layers.BatchNormalization()(conv5)\n",
        "    conv5 = layers.Add()([layers.Conv2D(1024, 1, padding='same', kernel_initializer='he_normal')(mpool4), conv5])  # Match channels\n",
        "    conv5 = layers.ReLU()(conv5)\n",
        "    conv5 = layers.Dropout(0.5)(conv5)\n",
        "\n",
        "    # Decoder\n",
        "    up6 = layers.Conv2DTranspose(512, 2, strides=2, kernel_initializer='he_normal', padding='same')(conv5)\n",
        "    up6 = layers.Concatenate()([up6, conv4])\n",
        "    conv6 = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(up6)\n",
        "    conv6 = layers.BatchNormalization()(conv6)\n",
        "    conv6 = layers.ReLU()(conv6)\n",
        "    conv6 = layers.Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv6)\n",
        "    conv6 = layers.BatchNormalization()(conv6)\n",
        "    conv6 = layers.Add()([layers.Conv2D(512, 1, padding='same', kernel_initializer='he_normal')(up6), conv6])  # Match channels\n",
        "    conv6 = layers.ReLU()(conv6)\n",
        "    conv6 = layers.Dropout(0.4)(conv6)\n",
        "\n",
        "    up7 = layers.Conv2DTranspose(256, 2, strides=2, kernel_initializer='he_normal', padding='same')(conv6)\n",
        "    up7 = layers.Concatenate()([up7, conv3])\n",
        "    conv7 = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(up7)\n",
        "    conv7 = layers.BatchNormalization()(conv7)\n",
        "    conv7 = layers.ReLU()(conv7)\n",
        "    conv7 = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv7)\n",
        "    conv7 = layers.BatchNormalization()(conv7)\n",
        "    conv7 = layers.Add()([layers.Conv2D(256, 1, padding='same', kernel_initializer='he_normal')(up7), conv7])  # Match channels\n",
        "    conv7 = layers.ReLU()(conv7)\n",
        "    conv7 = layers.Dropout(0.3)(conv7)\n",
        "\n",
        "    up8 = layers.Conv2DTranspose(128, 2, strides=2, kernel_initializer='he_normal', padding='same')(conv7)\n",
        "    up8 = layers.Concatenate()([up8, conv2])\n",
        "    conv8 = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(up8)\n",
        "    conv8 = layers.BatchNormalization()(conv8)\n",
        "    conv8 = layers.ReLU()(conv8)\n",
        "    conv8 = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv8)\n",
        "    conv8 = layers.BatchNormalization()(conv8)\n",
        "    conv8 = layers.Add()([layers.Conv2D(128, 1, padding='same', kernel_initializer='he_normal')(up8), conv8])  # Match channels\n",
        "    conv8 = layers.ReLU()(conv8)\n",
        "    conv8 = layers.Dropout(0.2)(conv8)\n",
        "\n",
        "    up9 = layers.Conv2DTranspose(64, 2, strides=2, kernel_initializer='he_normal', padding='same')(conv8)\n",
        "    up9 = layers.Concatenate()([up9, conv1])\n",
        "    conv9 = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(up9)\n",
        "    conv9 = layers.BatchNormalization()(conv9)\n",
        "    conv9 = layers.ReLU()(conv9)\n",
        "    conv9 = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv9)\n",
        "    conv9 = layers.BatchNormalization()(conv9)\n",
        "    conv9 = layers.Add()([layers.Conv2D(64, 1, padding='same', kernel_initializer='he_normal')(up9), conv9])  # Match channels\n",
        "    conv9 = layers.ReLU()(conv9)\n",
        "    conv9 = layers.Dropout(0.1)(conv9)\n",
        "\n",
        "    outputs = layers.Conv2D(5, 1, activation='softmax', kernel_initializer='he_normal')(conv9)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bhafFGGLnGQE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model_with_bn_rb()"
      ],
      "metadata": {
        "id": "WUcjD4mnX3c5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
        "    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
        "        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
        "        if labels_to_exclude is None:\n",
        "            labels_to_exclude = [0]  # Default to excluding label 0\n",
        "        self.labels_to_exclude = labels_to_exclude\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Convert predictions to class labels\n",
        "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
        "\n",
        "        # Flatten the tensors\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(y_pred, [-1])\n",
        "\n",
        "        # Apply mask to exclude specified labels\n",
        "        for label in self.labels_to_exclude:\n",
        "            mask = tf.not_equal(y_true, label)\n",
        "            y_true = tf.boolean_mask(y_true, mask)\n",
        "            y_pred = tf.boolean_mask(y_pred, mask)\n",
        "\n",
        "        # Update the state\n",
        "        return super().update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.AdamW(1e-3),\n",
        "    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0])]\n",
        ")"
      ],
      "metadata": {
        "id": "NrSyLcb7QcV9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "5FenjyQqXAwb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x = X_train,\n",
        "    y = y_train,\n",
        "    epochs = 100,\n",
        "    callbacks = early_stopping,\n",
        "    validation_data = (X_val, y_val),\n",
        "    verbose=1\n",
        ").history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR4WTVtuWALG",
        "outputId": "b7e55e6b-a2b7-4078-c7ab-2ecdf18b145c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m325/327\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.2735 - loss: 16.4371 - mean_iou: 0.0976"
          ]
        }
      ]
    }
  ]
}
